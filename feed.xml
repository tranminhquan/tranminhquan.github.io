<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tranminhquan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tranminhquan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-07-21T08:07:11+00:00</updated><id>https://tranminhquan.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Beginner to git? This post is for you</title><link href="https://tranminhquan.github.io/blog/2023/git-commands/" rel="alternate" type="text/html" title="Beginner to git? This post is for you"/><published>2023-01-27T11:25:00+00:00</published><updated>2023-01-27T11:25:00+00:00</updated><id>https://tranminhquan.github.io/blog/2023/git-commands</id><content type="html" xml:base="https://tranminhquan.github.io/blog/2023/git-commands/"><![CDATA[<h1 id="why-git">Why git?</h1> <p>Have you ever modify on a same file, say, Google docs, with your college? It may have conflicts when all of you modify in the same line. Yes, that’s where git comes to rescue</p> <p>Each time you and your colleges modify the file, the new version is created, we need to control it. Yes, that’s where git comes to rescue</p> <p>-&gt; Git is a solution for version control</p> <p><img src="/assets/img/git_commands/overview_git_1.png" alt="overview_git_1"/></p> <ul> <li>Dot represents the file at that time</li> <li> <p>Line connecting the dots represents the worflow of the file</p> </li> <li><strong>Line of GREEN dots</strong> is the “main” workflow</li> <li><strong>Line of BLUE dots</strong> is “your” workflow: when you modify the “main” workflow, you just <strong>PULL</strong> the “main” workflow to create your own workflow</li> <li> <p><strong>Line of ORANGE dots</strong> is “your college” workflow: same as you when he or she modify the “main”, he or she may <strong>PULL</strong> the “main” workflow to create his or her workflow</p> </li> <li>After completing modifcation, <strong>you MERGE</strong> it to the “main” workflow (BLUE line merges to GREEN line)</li> <li>After completing modification, <strong>your college MERGES</strong> it to the “main” workflow (ORANGE line merges to GREEN line)</li> <li>At the “main” worflow (GREEN line), it will be automatically aggregated from “your” and “your college” workflow</li> </ul> <p>That’s how GIT work</p> <p>So, where is the “main” workflow? It comes from a shared place specifically for code (just something like Google Drive or One Drive). Some popular services are Github, Gitlab</p> <p>Where is “your” worflow? It’s on your personal computer. The same for “your college” worflow. It’s on his or her computer.</p> <p>You can see that the above process requires two things: <strong>PULL</strong> and <strong>MERGE</strong></p> <ul> <li><strong>PULL</strong>: get the file from “main” worflow to your computer</li> <li><strong>MERGE</strong>: contains 2 steps: <strong>PUSH</strong> the file on your computer to the shared place (“main” workflow), and, <strong>MERGE</strong> means updating the “main” file. (Because you have modified the file, it needs to be merged to the current “main” file)</li> </ul> <p>Below is the demonstration of a simple file</p> <p><img src="/assets/img/git_commands/overview_git_2.png" alt="overview_git_2"/></p> <p>In summary, we have 2 sides: the shared place and the workplace on your computer</p> <ul> <li>The shared place is called <strong>REMOTE</strong></li> <li>The workplace of yours is called <strong>LOCAL</strong></li> </ul> <p><img src="/assets/img/git_commands/git_commands.png" alt="git_commands"/></p> <p>In the <strong>LOCAL</strong>, it is divided into 3 states:</p> <ul> <li>Working directory: where you are coding (e.g. VSCode), this place will NOT AFFECT the change of “your work”</li> <li>Staging: a “temperory” place to “index” the change of your working directory. This place will AFFECT the change of “your work”</li> <li>Local repository: your “workplace”, you will <strong>PUSH</strong> your work from here to the <strong>REMOTE</strong></li> </ul> <p>The <strong>REMOTE</strong> is the shared place, it can be placed on any familiar platform, two popular ones are <em>Github</em> and <em>Gitlab</em>. Its official name is <strong>repository</strong>, or (<strong>repo</strong> in short)</p> <p>Now supose your <strong>repo</strong> already existed and you are assigned to work together with your team on it.</p> <ul> <li>To “get” the code from <strong>repo</strong> to the <strong>local</strong>, use <strong>git pull</strong> command</li> <li>Then, you will modify the code on the Working directory, when you finish, you need to “update” it to the <strong>repo</strong> by $3$ steps: <ul> <li>Add <em>what changes you want</em> to the staging using <strong>git add</strong> command</li> <li>Move it to the <strong>local</strong> and ready for the <strong>repo&amp;&amp; by using **git commit</strong> command</li> <li>Officially push to the <strong>repo</strong> by using <strong>git push</strong> command</li> </ul> </li> </ul> <h1 id="summary">Summary</h1> <ul> <li> <p><strong>Repo</strong> is the <strong>remote</strong> place to storing code on. Some popular providers are <em>Github</em>, <em>Gitlab</em></p> </li> <li> <p><strong>Local</strong> is your workplace on your computer</p> </li> <li><strong>git pull</strong>: get the code from <strong>repo</strong> to <strong>local</strong></li> <li><strong>git add</strong>: add <em>changes</em> from what you have modified to the staging</li> <li><strong>git commit</strong>: confirm those <em>changes</em> that be ready to share on repo</li> <li><strong>git push</strong>: officially push tose <em>changes</em> to the <strong>repo</strong></li> </ul> <h1 id="git-in-practice">Git in practice</h1> <ul> <li>Initialize the repo from existing work</li> <li>Communicate with git when working</li> </ul> <h2 id="initialize-the-repo-from-existing-work">Initialize the repo from existing work</h2> <p>Suppose you have an existing work, and want to intialize a shared place on Github <img src="/assets/img/git_commands/existing_work.png" alt="existing_work"/></p> <p>In the termnial,</p> <ul> <li> <p>Initialize git by using <strong>git init</strong> commands. This will setup a local <strong>repository</strong> <img src="/assets/img/git_commands/git_init.png" alt="git_init"/></p> </li> <li> <p>Add <em>changes</em> you want by using <strong>git add</strong> <em>&lt;files or folders&gt;</em>. If you want to add *all changes<strong>, simply use **git add .</strong></p> <ul> <li>Option 1: If you want to add, e.g. these files, add these path using <strong>git add</strong>, you can verify the added changes by using <strong>git status</strong>, which are in green</li> </ul> <p><img src="/assets/img/git_commands/git_add_files.png" alt="git_add_files"/> <img src="/assets/img/git_commands/git_add_files_2.png" alt="git_add_files_2"/> <img src="/assets/img/git_commands/git_add_files_3.png" alt="git_add_files_3"/></p> <ul> <li>Option 2: If you want to add all files, use <strong>git add .</strong>, then use <strong>git status</strong> to verify, all files are in green</li> </ul> <p><img src="/assets/img/git_commands/git_add_all.png" alt="git_add_all"/></p> </li> <li> <p>To confirm those <em>changes</em> will be push to <strong>remote repo</strong>, use commands <strong>git commit -m</strong> <em>&lt;your note or comments&gt;</em></p> <p><img src="/assets/img/git_commands/git_commit_fail.png" alt="git_commit_fail"/></p> <p>If the above error occurs, that means you haven’t clarify yourself, then add your user name and email by using those commands. Note that only add the “–global” if you are using your personal device, if you are on a shared server, it is better to remove it.</p> <p><img src="/assets/img/git_commands/git_config_identification.png" alt="git_config_identification"/></p> <p>Then, use <strong>git commit</strong> again. Git will tell you all files are confirmed</p> <p><img src="/assets/img/git_commands/git_commit.png" alt="git_commit"/></p> </li> <li> <p>To officially push those <em>changes</em> to <strong>remote repo</strong>, you <strong>git push</strong>. However, you do not have a <strong>remote repo</strong> at this time. So, first, create a <strong>repo</strong> on Github</p> <p><img src="/assets/img/git_commands/create_repo.png" alt="create_repo"/></p> <p>You will see a https url of your repo, copy it. Back to the terminal, use <strong>git remote add origin</strong> <em>/&lt; https url /&gt;</em> to connect with the above repo</p> <p><img src="/assets/img/git_commands/git_remote_add.png" alt="git_remote_add"/></p> <p>Officially push to the remote repo by using <strong>git push -u origin master</strong> <img src="/assets/img/git_commands/git_push.png" alt="git_push"/></p> </li> </ul>]]></content><author><name></name></author><category term="fundamentals-engineering"/><category term="fundamentals"/><category term="inanutshell"/><category term="git"/><summary type="html"><![CDATA[Why git? Have you ever modify on a same file, say, Google docs, with your college? It may have conflicts when all of you modify in the same line. Yes, that’s where git comes to rescue]]></summary></entry><entry><title type="html">Bayesian Optimization</title><link href="https://tranminhquan.github.io/blog/2021/bayesia-opt/" rel="alternate" type="text/html" title="Bayesian Optimization"/><published>2021-01-30T00:00:00+00:00</published><updated>2021-01-30T00:00:00+00:00</updated><id>https://tranminhquan.github.io/blog/2021/bayesia-opt</id><content type="html" xml:base="https://tranminhquan.github.io/blog/2021/bayesia-opt/"><![CDATA[<h1 id="opening-discussion">Opening Discussion</h1> <p><img src="/assets/img/bayesian_opt/open_discussion.png" alt="open_discussion"/></p> <h1 id="levels-of-optimization-problem-solving">Levels of Optimization Problem Solving</h1> <p>In general, we can divided the way to solve optmization problem into $3$ levels:</p> <ul> <li><strong>Level 0</strong>: Non-derivative, including some familiar approaches such as: Grid Search, Random Search, Bayesian Optimization, etc. <ul> <li>Pros: Fast computation, do not require the function to be smooth, i.e. no differentiable required, the function need continous</li> <li>Cons: It is heuristic, not a theory-based method</li> </ul> </li> <li><strong>Level 1</strong>: First-derivative. $∇f(x) = f(x_i)_i$. The most popular one is Gradient Descent (Adam, Nesterov, etc.) <ul> <li>Pros: Efficient to converge in local minimum/maximum</li> <li>Cons: Computational since it requires backpropagation</li> </ul> </li> <li><strong>Level 2</strong>: Second-derivatie. $J(f) = f(x_i, x_j)_{ij}$ (i.e. Newton) <ul> <li>Pros: Very effective, faster convergence than level-1 method</li> <li>Cons: Significantly compuational</li> </ul> </li> </ul> <p>Bayesian Optimization is the one in level 0</p> <h1 id="bayesian-optimization">Bayesian Optimization</h1> <h2 id="when-to-use-bayesian-optimization">When to use Bayesian Optimization</h2> <p>Supose that we are optimzing a function $f(x, \Theta)$ \(\max_{\Theta}f(x,\Theta)\)</p> <p>We can consider Bayesian Optimzation if $f$ satisfy following conditions:</p> <ul> <li>$f$ has to be continuous</li> <li>$f$ is expensive to evaluate</li> <li>$f$ is a blackbox. In other words, we don’t know characteristic of $f$ (convex, non-convex, derivative, etc.)</li> </ul> <h2 id="fundementals-of-bayesian-optmization">Fundementals of Bayesian Optmization</h2> <p>Instead of directly optimizing $f$, we approximate it by other easier evaluation function $P$, and define a strategy $u$ to optimize $P$</p> <ul> <li>$P$ - <strong>Surrogate model</strong>: <ul> <li>the alternative of $f$, easier to optmize</li> <li>Gaussian Process, etc.</li> </ul> </li> <li>$u$ - <strong>Acquisition function</strong>: <ul> <li>strategy to optmize surrogate model</li> <li>Expected Improvement (EI), Upper Condidence Bound (UCB), etc.</li> </ul> </li> </ul> <p>In this article, we focus on the most popular surrogate model - the Gaussian Process</p> <h2 id="gaussian-process">Gaussian Process</h2> <p>As mentioned, $f$ is black box and expensive to evaluate. Hence, the surrogate model should be an alternative that is easier to evaluate. A good intuition is to approximate $f$ as a distribution (normally Gaussian Distribution).<br/> In other words, we consider $f$ as a Gaussian Distribution with mean $\mu(x)$ and standard deviation $\sigma^2(x)$</p> \[f(x) \sim \mathcal{N}(\mu(x), \sigma^2(x))\] <p>Hence, the Gaussian Process $P$ has the Probability Density Function (PDF) as</p> \[P(f(x)=y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\frac{|y - \mu(x)|^2}{2\sigma^2}}\] <p>The figure below demonstrates it as a distribution approximation </p> <p>Therefore, Gaussian Process makes use of Bayes Theory. Since we literally has no information about how $f$ looks like (<em>the solid red line</em>), we just has some initial observations (<em>blue circle</em>). <strong>The surrogate model gives us a probabilistic estimation of $f$ as a distribution (<em>the area limited by dash red line</em>)</strong></p> <p>From this, we need to select the next point (i.e. $x_+$) to sample from $f$. The strategy to select the next point from the observation of surrogate model is taken by <strong>acquisition function</strong>.</p> <p>After the next point is sampled, i.e. ${x_+, y=f(x_+)}$. We continue to fit it as prior knowledge to surrogate model and continue to select next point, i.e. $x_{++}$. Gradually, we will have a more knowledge on how $f$ looks like as the belief keeps increasing.</p> <h2 id="acquisition-function">Acquisition function</h2> <p>The strategy that proposes the next sampling point is decided by acquisition function $u$. Particularly, we optimize $u$ over surrogate model $P$. Note that as mentioned, those functions are easier to optimize comparing to $f$<br/> There are lots of <strong>acquisition function</strong>, the most popular ones are <strong>Expected Improvement</strong> $EI$, and <strong>Upper Confidence Bound</strong> $UCB$. In this article, we define $EI$ as the acquisition function.</p> <p>Expected Improvement function is defined as \(EI(x) = \mathbb{E} \max (x - f(x^*))\)</p> <p>The intuition is that the chosen next sampling point should have maximal expected values comparing the best of those from observations. The best point from existing observations is denoted as \(x^* = \arg\max_{x_i \in D} f(x_i)\)</p> <p>Since we are optimizing acquisition function over a Gaussian Process, the more particular formula can be clarified as \(EI(x) = \begin{cases} \displaystyle (\mu(x) - f(x^*) - \xi)\varphi(Z) + \sigma(x)\phi(Z) &amp; \text{if} &amp; \sigma(x) &gt; 0 \\ \displaystyle 0 &amp; \text{if} &amp; \sigma(x) = 0 \end{cases}\)</p> <p>where</p> \[Z = \begin{cases} \displaystyle \frac{\mu(x) - f(x^*) - \xi}{\sigma(x)} &amp; \text{if} &amp; \sigma(x) &gt; 0 \\ \displaystyle 0 &amp; \text{if} &amp; \sigma(x) = 0 \end{cases}\] <p>The adjusted parameter $\xi$ is use to balance between <em>exploitation</em> and <em>exploration</em> of two summation terms in the above equation, respectively.</p> <p>High $\xi$ means we lower the probability of first term (the one calculating with mean $\mu$, i.e. the certainty), hence, increase the probability of second term (the one calculating with variance $\sigma$, i.e. the uncertainty) –&gt; we want to explore on the area of uncertainty more, and vice versa.</p> <h2 id="the-complete-process">The complete process</h2> <p>In summary, the overall process of Bayesian Optimization can be clarified as follows: We want to find the optimal values of objective function $f$ with <strong>surrogate model</strong> $P$ and <strong>acquisition function</strong> $u$. Initially, we have limited observations $D={x_N, y_N}$. The iteration below is how Bayesian Optimization works:<br/> — <br/> 1. Fit ${x_N, y_N}$ to approximate surrogate model $P$<br/> 2. Optimize acquisition function $u$ over $P$ to sample the next point $x_{N+} = u(x)$<br/> 3. Evaluate ${x_{N+}}$ on $f$ to get $y_{N+}$<br/> 4. Add ${x_{N+}, y_{N+}}$ to D and repeat the process</p> <p>The detail about implementation from scratch can be found at <a href="http://krasserm.github.io/2018/03/19/gaussian-processes/">[4]</a></p> <p>Below is the demonstration step-by-step:</p> <p>Suppose we want to find the values that maximize the objective function below (which is unknown) with some initial observations. <img src="/assets/img/bayesian_opt/process_step1.png" alt="process_step1"/></p> <ol> <li> <p>Approximate a surrogate model $P$ (i.e. Gaussian Process). The green area is CI - Confidence Interval drawn from $\sigma^2$ of surrogate model, depicts the uncertainty over objective function <img src="/assets/img/bayesian_opt/process_step2.png" alt="process_step2"/></p> </li> <li> <p>Apply acquisition function $u$ over $P$. As a result, it proposed the next sampling point as blue circle <img src="/assets/img/bayesian_opt/process_step3.png" alt="process_step3"/></p> </li> <li> <p>Evaluate the proposed sampling point using objective function. We can observe that the uncertainty significantly narrow down. We repeat this process through a number of iterations <img src="/assets/img/bayesian_opt/process_step4.png" alt="process_step4"/></p> </li> </ol> <h2 id="discussion">Discussion:</h2> <p>We may have a general observation about Bayesian Optimization. However, there are still some that we have resolve yet:</p> <ul> <li>Alternatively, we optimize acquisition function over surrogate model instead of objective function since it cheaper, yet it is not clear the approach to optimize acquisition function</li> <li>Beside Expected Improvement, other popular acquisition functions are UCB, Probability of Improvement</li> </ul> <h1 id="implementation">Implementation</h1> <p>To simplify the implementation, we suppose that:</p> <ul> <li>We have already known the objective function for the verification.</li> <li>The surrogate model makes use of Gaussian Process Regression provided by <code class="language-plaintext highlighter-rouge">scikit-learn</code></li> <li>The process of optimizing the acquisition function will be handled by <code class="language-plaintext highlighter-rouge">scipy</code> employing <em>Broyden–Fletcher–Goldfarb–Shanno</em> algorithm[6]</li> </ul> <p>We will implement</p> <ul> <li>The Bayesian Optimization process</li> <li>A method maximizing acquisition function</li> <li>An instance of acquisition function, i.e. Expected Improvement</li> </ul> <h2 id="set-up">Set up</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TO-DO: problem with scipy &gt; 1.4, find out solution
</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">unistall</span> <span class="n">scipy</span> <span class="o">--</span><span class="n">y</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">scipy</span><span class="o">==</span><span class="mf">1.4</span><span class="p">.</span><span class="mi">1</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># assume the objective function f
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Initial samples
</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span>
<span class="n">X_inits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">]])</span>
<span class="n">y_inits</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">X_inits</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">X_inits</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_inits</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Bounds of x
</span><span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(2, 1) (2, 1)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># enrich samples to draw objective function
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="mf">0.01</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">noise</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">The objective function and initial observations: </span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">y--</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Objective function</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="p">),</span> <span class="sh">'</span><span class="s">gx</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Noisy samples</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_inits</span><span class="p">,</span> <span class="n">y_inits</span><span class="p">,</span> <span class="sh">'</span><span class="s">kx</span><span class="sh">'</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Initial samples</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The objective function and initial observations: 
</code></pre></div></div> <h2 id="bayesian-optimization-1">Bayesian Optimization</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># !wget !wget https://raw.githubusercontent.com/krasserm/bayesian-machine-learning/dev/bayesian-optimization/bayesian_optimization_util.py
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="n">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ConstantKernel</span><span class="p">,</span> <span class="n">Matern</span>
<span class="kn">from</span> <span class="n">bayesian_optimization_util</span> <span class="kn">import</span> <span class="n">plot_approximation</span><span class="p">,</span> <span class="n">plot_acquisition</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_bayesian_opt</span><span class="p">(</span><span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">gpr</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Single iteration of a Bayeisan Optimization process
    Args:
        X_samples: 
        y_samples: 
        gpr: 
        
    Returns:
        Next sampling pair {X, y}
    </span><span class="sh">"""</span>
    
    <span class="c1"># approximate GPR
</span>    <span class="n">gpr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">)</span>

    <span class="c1"># optimize acquisition over GPR to chose next sampling point X_next
</span>    <span class="n">X_next</span> <span class="o">=</span> <span class="nf">optimize_acquisition</span><span class="p">(</span><span class="n">EI</span><span class="p">,</span> <span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">n_restarts</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

    <span class="c1"># evaluate to get y_next
</span>    <span class="n">y_next</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">X_next</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">y_next</span>
</code></pre></div></div> <h2 id="optimize-acquisition">Optimize acquisition</h2> <p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"><code class="language-plaintext highlighter-rouge">scipy.optimize.minimize</code></a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="k">def</span> <span class="nf">optimize_acquisition</span><span class="p">(</span><span class="n">acquisition_func</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">n_restarts</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Optimize acquisition function
    Args:
        acquisition_func: 
        X_sample: 
        y_sample: 
        gpr: 
        bounds:
        n_restarts:
        
    Returns:
        proposed location
    </span><span class="sh">"""</span>
    
    <span class="n">dim</span> <span class="o">=</span> <span class="n">X_sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">min_value</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">min_x</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">minimize_objective</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="nf">acquisition_func</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">dim</span><span class="p">),</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">)</span>
    
    <span class="c1"># minimize by a popular algorithm
</span>    <span class="k">for</span> <span class="n">x0</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_restarts</span><span class="p">,</span> <span class="n">dim</span><span class="p">)):</span>
        <span class="n">min_ei</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">minimize_objective</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">L-BFGS-B</span><span class="sh">'</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">min_ei</span><span class="p">.</span><span class="n">fun</span> <span class="o">&lt;</span> <span class="n">min_value</span><span class="p">:</span>
            <span class="n">min_value</span> <span class="o">=</span> <span class="n">min_ei</span><span class="p">.</span><span class="n">fun</span>
            <span class="n">min_x</span> <span class="o">=</span> <span class="n">min_ei</span><span class="p">.</span><span class="n">x</span>
    
    <span class="k">return</span> <span class="n">min_x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="acquisition_func-expected-improvement"><code class="language-plaintext highlighter-rouge">acquisition_func</code>: Expected Improvement</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">EI</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Expected Improvement acquisition function
    Args:
        X
        X_sample: 
        y_sample: 
        gpr:
        xi: 
    
    Returns
        EI at X
    </span><span class="sh">"""</span>
    
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">gpr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">mu_sample</span> <span class="o">=</span> <span class="n">gpr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>
    
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># find f(x^*)
</span>    <span class="n">mu_max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">mu_sample</span><span class="p">)</span>
    
    <span class="c1"># EI formula
</span>    <span class="k">with</span> <span class="n">np</span><span class="p">.</span><span class="nf">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="sh">'</span><span class="s">warn</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">mu_dst</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">mu_max</span> <span class="o">-</span> <span class="n">xi</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">mu_dst</span> <span class="o">/</span> <span class="n">sigma</span>
        <span class="n">ei</span> <span class="o">=</span> <span class="n">mu_dst</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        <span class="n">ei</span><span class="p">[</span><span class="n">sigma</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
    
    <span class="k">return</span> <span class="n">ei</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m52</span> <span class="o">=</span> <span class="nc">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="nc">Matern</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">gpr</span> <span class="o">=</span> <span class="nc">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">X_samples</span> <span class="o">=</span> <span class="n">X_inits</span>
<span class="n">y_samples</span> <span class="o">=</span> <span class="n">y_inits</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>    
    <span class="n">X_next</span><span class="p">,</span> <span class="n">y_next</span> <span class="o">=</span> <span class="nf">process_bayesian_opt</span><span class="p">(</span><span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">gpr</span><span class="p">)</span>
    
    <span class="c1"># Plot samples, surrogate function, noise-free objective and next sampling location
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">n_iters</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nf">plot_approximation</span><span class="p">(</span><span class="n">gpr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Iteration </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">n_iters</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="nf">plot_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nc">EI</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">gpr</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># Add sample to previous samples
</span>    <span class="n">X_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">X_samples</span><span class="p">,</span> <span class="n">X_next</span><span class="p">))</span>
    <span class="n">y_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">y_samples</span><span class="p">,</span> <span class="n">y_next</span><span class="p">))</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">bayesian_optimization_util</span> <span class="kn">import</span> <span class="n">plot_convergence</span>

<span class="nf">plot_convergence</span><span class="p">(</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">)</span>
</code></pre></div></div> <h1 id="references">References</h1> <p>[1] A Tutorial on Bayesian Optimization: https://arxiv.org/pdf/1807.02811.pdf<br/> [2] <a href="https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0">The intuitions behind Bayesian Optimization with Gaussian Processes</a><br/> [3] <a href="https://orbi.uliege.be/bitstream/2268/226433/1/PyData%202017_%20Bayesian%20optimization%20with%20Scikit-Optimize.pdf">Bayesian optimization with Scikit-Optimize</a><br/> [4] <a href="http://krasserm.github.io/2018/03/21/bayesian-optimization/">Bayesian Optimization</a><br/> [5] <a href="http://krasserm.github.io/2018/03/19/gaussian-processes/">Gaussian processes</a><br/> [6] <a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">Broyden–Fletcher–Goldfarb–Shanno algorithm</a></p>]]></content><author><name></name></author><category term="probabilistic-ml"/><category term="paramsearch"/><category term="bayes"/><category term="probml"/><summary type="html"><![CDATA[Opening Discussion]]></summary></entry><entry><title type="html">How to set up a basic GPU environment on Google Cloud Platform</title><link href="https://tranminhquan.github.io/blog/2019/how-to-set-up-a-basic-gpu-environment-on-google-cloud-platform/" rel="alternate" type="text/html" title="How to set up a basic GPU environment on Google Cloud Platform"/><published>2019-02-17T14:31:00+00:00</published><updated>2019-02-17T14:31:00+00:00</updated><id>https://tranminhquan.github.io/blog/2019/how-to-set-up-a-basic-gpu-environment-on-google-cloud-platform</id><content type="html" xml:base="https://tranminhquan.github.io/blog/2019/how-to-set-up-a-basic-gpu-environment-on-google-cloud-platform/"><![CDATA[]]></content><author><name></name></author></entry></feed>