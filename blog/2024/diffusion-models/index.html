<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Diffusion Models: Fundamentals - Part 1 | Quan M. Tran</title> <meta name="author" content="Quan M. Tran"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="quantran, tranminhquan, deeplearning-note, quanmtran"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://tranminhquan.github.io/blog/2024/diffusion-models/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Diffusion Models: Fundamentals - Part 1",
      "description": "",
      "published": "January 2, 2024",
      "authors": [
        {
          "author": "Quan Tran",
          "authorURL": "",
          "affiliations": [
            {
              "name": "RnD Department, Kyanon Digital",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Quan </span>M. Tran</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Diffusion Models: Fundamentals - Part 1</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#what-is-diffusion-models">What is Diffusion Models</a></div> <div><a href="#math-formulation">Math formulation</a></div> <div><a href="#overall-training-process">Overall training process</a></div> <div><a href="#model-backbone">Model backbone</a></div> <div><a href="#summary">Summary</a></div> <div><a href="#implementation">Implementation</a></div> </nav> </d-contents> <p>Update history</p> <ul> <li>20.12.2023: Created</li> <li>02.01.2024: Update the math formulation</li> <li>03.01.2024: Update the model backbone</li> <li>04.01.2024: Update the implementation</li> </ul> <h2 id="what-is-diffusion-model">What is Diffusion Model</h2> <ul> <li>Motivation <ul> <li>From VAE: the same concept but gradually</li> <li>From GAN: the same concept that generate from noise</li> </ul> </li> <li>General concept <ul> <li>Markov Chain with 2 stages: forward and reverse</li> <li>Forward process</li> <li>Reverse process</li> </ul> </li> <li>Objective function <ul> <li>Learn to generate the image from noise</li> </ul> </li> <li>Compare <ul> <li>to GAN: more stable in training</li> <li>to VAE:</li> </ul> </li> </ul> <h3 id="general-concept">General concept</h3> <p>The general objective is to gradually adding noise to the original image unitl it completely becames noise, and then try to generate back to the original image from the existing noise. By learning this process, a diffusion model is expected to learn how to generate an image from a noise distribution by learning the denoising process.</p> <p>Diffusion model is a Markov Chain process with two stages: forward and reverse.</p> <ul> <li>Forward process: <ul> <li>In the forward process, the objective is to adding noise to the image within finite steps until it completely becomes noise</li> </ul> </li> <li>Reverse process: <ul> <li>In the reverse process, the model learns to denoise from a noise distribution and generate back the original image within finite steps</li> </ul> </li> <li>Objective function: <ul> <li>Similar to VAE, diffusion model optimizes the evidence lower bound (ELBO), which means we want to maximize the log-likelihood between the original image and the generated one.</li> </ul> </li> </ul> <h2 id="math-formulation">Math formulation</h2> <ul> <li>We denote the real data distribution as \(q(x)\)</li> <li>A data point (real image) is sampled as \(\mathbf{x}_0 \sim q(\mathbf{x})\)</li> <li>The Markov chains is defined as \(T\) finite steps</li> </ul> <h3 id="forward-process">Forward process</h3> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion_models/forward_process-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion_models/forward_process-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion_models/forward_process-1400.webp"></source> <img src="/assets/img/diffusion_models/forward_process.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li>In the forward process, at timestep t, we have the noised image \(\mathbf{x}_t\) by adding noise from a distribution (normally Gaussian) to the previous image \(\mathbf{x}_{t-1}\), this process is denoted as \(q(\mathbf{x}_t \vert \mathbf{x}_{t-1})\)</li> </ul> <p>Formally,</p> \[\begin{equation} \label{eq:original_q_forward} q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}) \end{equation}\] <p>where \(\beta_{t}\) is a paramter to control the level of noise at timestep \(t\), \(\{\beta_t \in (0,1)\}_{t=1}^{T}\).</p> <p>Thanks to the property of the Markov chain, we can create a tractable closed. Given \(\alpha_t = 1 - \beta_t\), \(\bar{\alpha_t} = \prod_{i=1}^{t} \alpha_i\), then</p> \[\begin{equation} \label{eq:closed_form} \begin{aligned} \mathbf{x}_t &amp;= \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\ &amp;= \sqrt{1 - \alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar\epsilon_{t-2} \\ &amp;= \dots \\ &amp;= \sqrt{\bar\alpha_t} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_t} \epsilon \end{aligned} \end{equation}\] <p>On the above equation \eqref{eq:closed_form}, we denote \(\epsilon_t \in \mathcal{N}(0, \mathbf{I})\). \(\bar\epsilon_{t}\) is a merged Gaussian distribution.</p> <p>From that, the closed form of forward process is defined as</p> \[\begin{equation} \label{eq:closed_form_forward} q(\mathbf{x}_t | \mathbf{x}_{0}) = \mathcal{N} \left( \mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}, (1 - \bar{\alpha}_t) \mathbf{I} \right) \end{equation}\] <h3 id="reverse-process">Reverse process</h3> <ul> <li>Theoretically, the reverse process would be \(q(\mathbf{x}_{t-1} \vert \mathbf{x}_{t})\).</li> <li>However, to find out the above distribution, we need to figure out the wholte data distribution, which is impractical</li> <li>The alternative is the reparameterizatin trick, sample from mean and distribution of the previous distribution. The approximation is \(p_{\theta}(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\).</li> <li>Because \(q(\mathbf{x}_{t-1} \vert \mathbf{x}_{t})\) is a Gaussian distribution, with a small \(\beta_t\), \(p_{\theta}(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\). is also a Gaussian distribution, so we can sample using the above clarification.</li> </ul> <p>Formally, \(\begin{equation} p_{\theta}(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N} \left( \mathbf{x}_{t}; \mathbf{\mu}_{\theta}(\mathbf{x}_t, t), \mathbf{\sum}_{\theta}(\mathbf{x}_t, t) \right) \end{equation}\)</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion_models/reverse_process-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion_models/reverse_process-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion_models/reverse_process-1400.webp"></source> <img src="/assets/img/diffusion_models/reverse_process.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="objective-function">Objective function</h3> <p>We optimize the ELBO function on the negative log-likelihod function.</p> \[\begin{equation} \begin{aligned} \mathbb{E}[-\log p_{\theta}(\mathbf{x_0})] &amp;\leq \mathbb{E_q}[-\log \frac{p_{\theta}(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)}] \\ &amp;= \mathbb{E_q}[-\log p(\mathbf{x}_T) - \sum_{t \geq 1} \frac{ p_{\theta}(\mathbf{x}_{t-1} \vert \mathbf{x}_t)} {q(\mathbf{x}_{t} \vert \mathbf{x}_{t-1})} ] \\ &amp;=: L \end{aligned} \end{equation}\] <p>Derive the above function, we get</p> \[\begin{equation} \label{eq:original_loss} L := \mathbb{E_q} \left[ \underbrace{D_{KL} \left( q(\mathbf{x}_T \vert \mathbf{x}_0) \enspace \vert\vert \enspace p_{\theta}(\mathbf{x}_T) \right) _ {L_T}} _{L_T} + \underbrace{\sum\limits_{t \geq 1} D_{KL} \left( q(\mathbf{x}_{t-1} \vert \mathbf{x}_{t}, \mathbf{x}_{0}) \enspace \vert\vert \enspace p_{\theta}(\mathbf{x}_{t-1} \vert \mathbf{x}_{t}) \right) } _{L_{t-1}} - \underbrace{\log p_{\theta} (\mathbf{x}_0 \vert \mathbf{x}_1)} _{L_0} \right] \end{equation}\] <p>In the above training loss, we observer there are three parts:</p> <ul> <li>\(L_T\) is a constant, and can be ignored during the training since \(q\) has no learnable parameters and \(\mathbf{x}_t\) is a Gaussian noise</li> <li>\(L_0\) is a reconstruction term and is learned using a seperate decoder following \(\mathcal{N} \sim (\mathbf{x}_0; \mu_\theta(\mathbf{x}_1, 1), \Sigma_\theta (\mathbf{x}_1, 1) )\)</li> <li>\(L_{t-1}\) is a learnable parameters and we focus on how to learn this subloss function.</li> </ul> <p>In \(L_{t-1}\), the term \(q(\mathbf{x}_{t-1} \vert \mathbf{x}_{t}, \mathbf{x}_{0})\) has not been defined. In words, it means we wish to denoise the image from previous noisy one \(\mathbf{x}_{t}\) and it is also conditioned on the original image \(\mathbf{x}_0\). As a result,</p> \[\begin{equation} \label{eq:original_reverse} q(\mathbf{x}_{t-1} \vert \mathbf{x}_{t}, \mathbf{x}_{0}) \sim \mathcal{N}(\mathbf{x}_{t-1}, \tilde\mu_t(\mathbf{x}_{t}, \mathbf{x}_0), \tilde\beta_{t} \mathbf{I}) \end{equation}\] <p>with</p> \[\begin{equation} \tilde\beta_t = \frac{1- \bar\alpha_{t-1}}{1 - \bar\alpha_t} \cdot \beta_t \end{equation}\] <p>Using the Bayes rules, \(\tilde\mu_t(\mathbf{x}_t, \mathbf{x}_0)\) in Equation \eqref{eq:original_reverse} can be derived into</p> \[\begin{equation} \label{eq:original_mu_noise} \tilde\mu_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\bar\alpha_{t-1}} \beta_t}{1 - \bar\alpha_t} \mathbf{x}_0 + \frac{\sqrt\alpha_t (1 - \bar\alpha_{t-1})}{1 - \bar\alpha_t} \mathbf{x}_t \end{equation}\] <p>However, it still depends on two variables, \(\mathbf{x}_0\) and \(\mathbf{x}_t\), we want to transform it to only depends on one variable. Because we have a tractable closed-form of \(\mathbf{x}_0\) and \(\mathbf{x}_t\) and \(\epsilon_t \sim \mathcal{N}(0, \mathbf{I})\) in Equation \eqref{eq:closed_form}, the Equation \eqref{eq:original_mu_noise} becomes</p> \[\begin{equation} \label{eq:mu_noise} \tilde\mu_t(\mathbf{x}_t) = \frac{1}{\sqrt{\alpha_t}} ( \mathbf{x}_{t} - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}} \epsilon_t ) \end{equation}\] <p>Recall the \(p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\) derive formula in Equation \eqref{eq:original_loss}, we can have the same transformation with a learnable \(\epsilon_\theta(\mathbf{x}_t, t)\) for \(\mu_\theta(\mathbf{x}_t, t)\)</p> \[\begin{equation} \label{eq:mu_learnable_noise} \mu_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} ( \mathbf{x}_{t} - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}} \epsilon_\theta(\mathbf{x}_t, t) ) \end{equation}\] <p>From the above equation, we observe that the generated \(\mathbf{x}_t\) depends only on a trainable variable, which is \(\epsilon_\theta(\mathbf{x}_t, t)\), at timestep \(t\). The problem turns out to predict the noise of the generated image for every step \(t\) in the denoising process. As a result, we define a neural network to predict \(\epsilon_\theta(\mathbf{x}_t, t)\)</p> <p>Applying \eqref{eq:mu_noise} and \eqref{eq:mu_learnable_noise} into the \(L_{t-1}\), now the objective is to minimize the difference between the current noise and the predicted noise</p> \[\begin{equation} \begin{aligned} L_{t} &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{1}{2 || \Sigma_\theta (\mathbf{x}_t, t) ||_2^2} || \tilde\mu_t(\mathbf{x}_t, \mathbf{x}_0) - \mu_\theta(\mathbf{x}_t, t) ||^2 \right] \\ &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{(1 - \alpha_t)^2}{2 \alpha_t (1 - \bar\alpha_t) || \Sigma_\theta ||_2^2} || \epsilon_t - \epsilon_\theta(\sqrt{\alpha_t} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_t} \epsilon_t, t) ||^2 \right] \end{aligned} \end{equation}\] <p>By discarding the regulaization term, Ho et al. proposed with a simpler version <d-cite key="ho2020_denoising"></d-cite></p> \[\begin{equation} \label{eq:simple_loss_function} L_{t}^{\text{simple}} = \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ || \epsilon_t - \epsilon_\theta(\sqrt{\alpha_t} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_t} \epsilon_t, t) ||^2 \right] \end{equation}\] <p>Since we ignore the other parts of the overall loss function and with the simple version, the final loss function is</p> \[\begin{equation} L_{\text{simple}} = L_{t}^{\text{simple}} + C \end{equation}\] <p>where \(C\) is a constant</p> <details><summary>Explanation for the impractical of \(q(\mathbf{x}_{t-1} \vert \mathbf{x}_{t})\)</summary> <p>TODO</p> </details> <details><summary>Explanation for the reparameterization trick</summary> <p>TODO</p> </details> <details><summary>Bayes rule to expand the \(q(\mathbf{x}_{t} \vert \mathbf{x}_{t-1}, \mathbf{x}_{0})\)</summary> <p>TODO</p> </details> <h2 id="overall-training-process">Overall training process</h2> <p>The process of developing diffusion model consitst of training and sampling.</p> <h3 id="training">Training</h3> <p>For each training step:</p> <ul> <li>Sample an original image from the dataset: \(\mathbf{x}_0 \sim q(\mathbf{x}_0)\)</li> <li>Sample range of timestep \(t\) in range (1, \(T\)), for example, sampling with a uniform: \(t \sim \text{Uniform}(\{1, \dots, T\})\)</li> <li>Sample noise: \(\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</li> <li>Calculate the gradient folllowing the loss function: \(\Delta_\theta || \epsilon - \epsilon_\theta(\sqrt{\alpha_t} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_t} \epsilon_t, t)||^2\)</li> </ul> <h3 id="sampling">Sampling</h3> <p>The sampling process is when we want to generate the image from the noise distribution.</p> <ul> <li>First, we generate the noise: \(\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</li> <li>For each timestep from \(T\) to \(1\) <ul> <li>Sample embedding from the noise distribution: \(\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</li> <li>Calculate the denoising version at timestep $t-1$ using the reparameterization trick:</li> </ul> \[\mathbf{x}_{t-1} \sim p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) \\ \mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_{t-1} - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t} \epsilon_\theta(\mathbf{x}_t, t)} + \sigma_t \mathbf{z} \right)\] </li> <li>Get \(\mathbf{x}_0\)</li> </ul> <p>The two process are summarized as follow Algorithms</p> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion_models/training_sampling_algorithms-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion_models/training_sampling_algorithms-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion_models/training_sampling_algorithms-1400.webp"></source> <img src="/assets/img/diffusion_models/training_sampling_algorithms.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="model-backbone">Model backbone</h2> <p>Any model can be used as the backbone for a diffusion model. However, for the baseline Denoising Diffusion Probabilistic Model (DDPM), Ho et al. <d-cite key="ho2020_denoising"> propose to leverage U-Net as the backbone.</d-cite></p> <p>The advatange of U-Net architecture can be listed as follows:</p> <ul> <li>U-Net is a symetric architecture and is well-known for its application in segmentation. This means the architecture itself is potential for denoising tasks.</li> <li>The conventional structure of U-Net has encoder as the downsampling and decoder as the upsampling, with residual connection, which is similar to Auto-Encoder-based models.</li> <li>U-Net has many variants, the recent famous one is Attenion U-Net consisting of Wide Resnet blocks, Group Normalization, and Self-attention Blocks.</li> <li>However, to leverage the U-Net as the backbone, we need to differentiate between each timestep t. This can be resolved by using a Position Encoding. In <d-cite key="ho2020_denoising">, the authors use SinusoidalPositionEmbeddings.</d-cite> </li> </ul> <div class="l-body-outset"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/diffusion_models/attention_unet-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/diffusion_models/attention_unet-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/diffusion_models/attention_unet-1400.webp"></source> <img src="/assets/img/diffusion_models/attention_unet.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="summary">Summary</h2> <h2 id="implementation">Implementation</h2> <h3 id="training-process">Training process</h3> <p>Following the above algorithm</p> <ul> <li>Sample an original image from the dataset: \(\mathbf{x}_0 \sim q(\mathbf{x}_0)\).</li> <li>Sample range of timestep \(t\) in range \((1, T)\), for example, sampling with a uniform: \(t \sim \text{Uniform}(\{1, \dots, T\})\). In this step, we generate range of timestep with corresponding \(\beta_t\).</li> <li>Sample noise: \(\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\). In this step, we sample noise and generate the noisy image at timestep \(t\) following Equation \eqref{eq:closed_form_forward}.</li> <li>Calculate the gradient folllowing the loss function: \(\Delta_\theta || \epsilon - \epsilon_\theta(\sqrt{\alpha_t} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_t} \epsilon_t, t)||^2\). In this step, we calculate between the output of the model and the noisy image from the above step using Equation \eqref{eq:simple_loss_function}</li> </ul> <p>As a result, in summary, we will implement the process as follow:</p> <ul> <li>Timestep and corresponding beta scheduler given number of steps</li> <li>Sample a noisy image at a timestep \(t\) with Equation \eqref{eq:closed_form_forward}.</li> <li>Define the loss function to calculate Equation \eqref{eq:simple_loss_function}</li> </ul> <h4 id="timestep-and-beta-scheduler">Timestep and beta scheduler</h4> <p>The importance of forward process is to define how we generate number of finite timestep in range \((0, T)\). The original DDPM paper uses the linear schedule for simple.</p> <p>Recall Equation \eqref{eq:original_q_forward} with \(\beta_t\) as the parameter to control the level of noise. In the original implementation, the authors choose to scale linearly from \(\beta_1 = 10^{-4}\) to \(\beta_T = 0.02\). So we implement the same way.</p> <p>We define <code class="language-plaintext highlighter-rouge">linear_beta_schedule</code> as the linear timestep scheduler with an input as the number of timesteps <code class="language-plaintext highlighter-rouge">n_steps</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">linear_beta_schedule</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">beta_start</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">0.02</span>
    
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">beta_start</span><span class="p">,</span> <span class="n">beta_end</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">)</span>
</code></pre></div></div> <p>We also need to prepare corresponding \(\alpha_t\) and \(\bar\alpha_t\). The following code prepare the list of <code class="language-plaintext highlighter-rouge">alpha</code> and utilization function to retrieve given timesteps</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define alphas
</span><span class="n">alphas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">betas</span>
<span class="n">cumprod_alphas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># bar_alpha
</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># util function to retrive data at timestep t
</span>
<span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Extract from the list of data the t-element and reshape to the given_shape

    Args:
        data (list or tensor): input list of tensor of data to retrieve
        t (int): t element to retrieve data
        out_shape (tuple): output shape

    Returns:
        tensor: retrieved data with dedicated shape
    </span><span class="sh">"""</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="nf">cpu</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">*</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">out_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))).</span><span class="nf">to</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <h4 id="sample-a-noisy-image-at-a-timestep">Sample a noisy image at a timestep</h4> <p>Recall the Equation \eqref{eq:closed_form_forward}</p> \[q(\mathbf{x}_t | \mathbf{x}_{0}) = \mathcal{N} \left( \mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}, (1 - \bar{\alpha}_t) \mathbf{I} \right)\] <p>We implement a function to get noised image at any given timestep with given original image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_noised_image</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Sample a noised image at a timestep

    Args:
        x_start (tensor): x0, original image
        timestep (_type_): timestep t
        noise (_type_, optional): noise type. Defaults to None.
    </span><span class="sh">"""</span>
    
    <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>
    
    <span class="n">cumprod_alpha_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">cumprod_alphas</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="n">noised_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">cumprod_alpha_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_start</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cumprod_alpha_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">noised_t</span>
    
</code></pre></div></div> <h4 id="define-the-transform-and-inverse-transform-process">Define the transform and inverse transform process</h4> <p>The transform process takes original image from PIL and transform to torch tensor data</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">transform</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span>
    <span class="nc">Resize</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
    <span class="nc">CenterCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
    <span class="nc">ToTensor</span><span class="p">(),</span> <span class="c1"># turn into torch Tensor of shape CHW, divide by 255
</span>    <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    
<span class="p">])</span>
</code></pre></div></div> <p>The inverse transform process convert the tensor data back to the PIL image</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reverse_transform</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span>
     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span>
     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="c1"># CHW to HWC
</span>     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span> <span class="o">*</span> <span class="mf">255.</span><span class="p">),</span>
     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)),</span>
     <span class="nc">ToPILImage</span><span class="p">(),</span>
<span class="p">])</span>
</code></pre></div></div> <h4 id="demo-the-forwarding-process">Demo the forwarding process</h4> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/demo_forwarding_diffusion_models.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <h4 id="define-the-loss-function">Define the loss function</h4> <p>From Equation \eqref{eq:simple_loss_function}, we can apply any conventional function such as L1, MSE, etc. to calculate the different between noised image at timestep \(t\) and the predicted noise from the model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">denoise_model</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="sh">'</span><span class="s">mse</span><span class="sh">'</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>
        
    <span class="n">x_noise</span> <span class="o">=</span> <span class="nf">sample_noised_image</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
    <span class="n">predicted_noise</span> <span class="o">=</span> <span class="nf">denoise_model</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">timestep</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mse</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">x_noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">NotImplementedError</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div> <p>The above code takes <code class="language-plaintext highlighter-rouge">denoise_model</code> into account, which is neural network that we will implement. The model will predict the noise at timestep \(t\) given the original image. In the next section, we will implement the model as the Attention U-Net.</p> <h3 id="attention-u-net">Attention U-Net</h3> <p>There are modules we need to implement</p> <ul> <li>Positional Embeddings</li> <li>ResNet Block</li> <li>Attention Block</li> <li>Group Normalization</li> <li>Utils Blocks</li> </ul> <h4 id="positional-embeddings">Positional Embeddings</h4> <p>For each above timestep \(t\), we need to generate a positional embedding to differentiate between each timestep. The most common is to follow <d-cite key="vaswani2017_attention"> with Sinusodial Positional Embedding.</d-cite></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SinusodialPositionalEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">timestep</span><span class="p">.</span><span class="n">device</span>
        <span class="n">half_dim</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">timestep</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">sin</span><span class="p">(),</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">cos</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">embeddings</span>
</code></pre></div></div> <h4 id="resnet-block">ResNet block</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A unit block in ResNet module. Each block consists of a projection module, a group normalization, and an activation

    Args:
        nn (_type_): _description_
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">act_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">SiLU</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="nf">act_fn</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale_shift</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">scale_shift</span><span class="p">:</span>
            <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span> <span class="o">=</span> <span class="n">scale_shift</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">scale</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">shift</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act_fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ResNetBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">act_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">SiLU</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="nf">act_fn</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">out_dim</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">time_emb_dim</span>
            <span class="k">else</span> <span class="bp">None</span>
        <span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">block1</span> <span class="o">=</span> <span class="nc">Block</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">act_fn</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block2</span> <span class="o">=</span> <span class="nc">Block</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">act_fn</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">res_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">in_dim</span> <span class="o">!=</span> <span class="n">out_dim</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time_emb</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">scale_shift</span> <span class="o">=</span> <span class="bp">None</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="ow">and</span> <span class="n">time_emb</span><span class="p">:</span>
            <span class="n">time_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">time_emb</span><span class="p">)</span>
            <span class="n">time_emb</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">time_emb</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c -&gt; b c 1 1</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">scale_shift</span> <span class="o">=</span> <span class="n">time_emb</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale_shift</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        
        
        <span class="k">return</span> <span class="n">h</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">res_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h4 id="attention-block">Attention Block</h4> <p>We follow the implementation of the paper <d-cite key="vaswani2017_attention"></d-cite></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">einsum</span>

<span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="o">-</span><span class="mf">0.5</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="n">heads</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">qkv_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">qkv_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">t</span> <span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="sh">"</span><span class="s">b (h c) x y -&gt; b h c (x y)</span><span class="sh">"</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">),</span> <span class="n">qkv</span>
        <span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scale</span>
        
        <span class="n">sim</span> <span class="o">=</span> <span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">b h d i, b h d j -&gt; b h i j</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">sim</span> <span class="o">-</span> <span class="n">sim</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">b h i j, b h d j -&gt; b h i d</span><span class="sh">"</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="sh">"</span><span class="s">b h (x y) d -&gt; b (h d) x y</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_proj</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div> <h4 id="group-normalization">Group Normalization</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>
        <span class="n">self</span><span class="p">.</span><span class="n">groupnorm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">groupnorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h4 id="utils-blocks">Utils Blocks</h4> <p>Residual</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Residual</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>
</code></pre></div></div> <p>Downsample block</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Downsample</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span> <span class="k">if</span> <span class="n">out_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">in_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">Rearrange</span><span class="p">(</span><span class="sh">"</span><span class="s">b c (h p1) (w p2) -&gt; b (c p1 p2) h w</span><span class="sh">"</span><span class="p">,</span> <span class="n">p1</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>Upsample block</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Upsample</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span> <span class="k">if</span> <span class="n">out_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">in_dim</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">up_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">nearest</span><span class="sh">'</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h4 id="the-whole-model">The whole model</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AttUNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> 
                 <span class="n">init_dim</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> 
                 <span class="n">out_dim</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> 
                 <span class="n">dim_mults</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                 <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">self_condition</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">resnet_block_groups</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_condition</span> <span class="o">=</span> <span class="n">self_condition</span>
        <span class="n">input_channels</span> <span class="o">=</span> <span class="n">channels</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="k">if</span> <span class="n">self_condition</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">init_dim</span> <span class="o">=</span> <span class="n">init_dim</span> <span class="k">if</span> <span class="n">init_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">init_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">init_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">init_dim</span><span class="p">,</span> <span class="o">*</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span> <span class="p">:</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">m</span><span class="p">,</span> <span class="n">dim_mults</span><span class="p">)]</span>
        <span class="n">in_out</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
        
        <span class="n">block_klass</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span><span class="n">ResNetBlock</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">resnet_block_groups</span><span class="p">)</span>
        
        <span class="c1"># positional embedding
</span>        <span class="n">time_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">SinusodialPositionalEmbeddings</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">downs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ups</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([])</span>
        <span class="n">num_resolutions</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">in_out</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">in_out</span><span class="p">):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="n">ind</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">num_resolutions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="n">self</span><span class="p">.</span><span class="n">downs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
                    <span class="nf">block_klass</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                    <span class="nf">block_klass</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                    <span class="nc">Residual</span><span class="p">(</span><span class="nc">GNorm</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">in_dim</span><span class="p">))),</span>
                    <span class="nc">Downsample</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_last</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="p">])</span>
            <span class="p">)</span>
            
        <span class="n">mid_dim</span> <span class="o">=</span> <span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_block1</span> <span class="o">=</span> <span class="nf">block_klass</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">mid_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_attention</span> <span class="o">=</span> <span class="nc">Residual</span><span class="p">(</span><span class="nc">GNorm</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_block2</span> <span class="o">=</span> <span class="nf">block_klass</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">mid_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">reversed</span><span class="p">(</span><span class="n">in_out</span><span class="p">)):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="n">ind</span> <span class="o">==</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">in_out</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="n">self</span><span class="p">.</span><span class="n">ups</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
                    <span class="nf">block_klass</span><span class="p">(</span><span class="n">out_dim</span> <span class="o">+</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                    <span class="nf">block_klass</span><span class="p">(</span><span class="n">out_dim</span> <span class="o">+</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                    <span class="nc">Residual</span><span class="p">(</span><span class="nc">GNorm</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">out_dim</span><span class="p">))),</span>
                    <span class="nc">Upsample</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_last</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="p">])</span>
            <span class="p">)</span>
            
        <span class="n">self</span><span class="p">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span> <span class="k">if</span> <span class="n">out_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">channels</span>
        
        
        <span class="n">self</span><span class="p">.</span><span class="n">final_res_block</span> <span class="o">=</span> <span class="nf">block_klass</span><span class="p">(</span><span class="n">dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">final_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">x_self_cond</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">self_condition</span><span class="p">:</span>
            <span class="n">x_self_cond</span> <span class="o">=</span> <span class="n">x_self_cond</span> <span class="k">if</span> <span class="n">x_self_cond</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x_self_cond</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">init_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
        
        <span class="n">t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">time_mlp</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>
        
        <span class="n">h</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">downsample</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">downs</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">h</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">h</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="n">x</span> <span class="o">=</span> <span class="nf">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">upsample</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">ups</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">.</span><span class="nf">pop</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">.</span><span class="nf">pop</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
            <span class="n">x</span> <span class="o">=</span> <span class="nf">upsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">final_res_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">final_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/diffusion-models.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"tranminhquan/tranminhquan.github.io","data-repo-id":"R_kgDOJ8OIkw","data-category":"Q&A","data-category-id":"DIC_kwDOJ8OIk84CX792","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Quan M. Tran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>