<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Bayesian Optimization | Quan M. Tran</title> <meta name="author" content="Quan M. Tran"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="quantran, tranminhquan, deeplearning-note, quanmtran"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://tranminhquan.github.io/blog/2021/bayesia-opt/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Bayesian Optimization",
      "description": "",
      "published": "January 30, 2021",
      "authors": [
        {
          "author": "Quan Tran",
          "authorURL": "",
          "affiliations": [
            {
              "name": "RnD Department, Kyanon Digital",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Quan </span>M. Tran</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Bayesian Optimization</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#opening-discussion">Opening Discussion</a></div> <div><a href="#levels-of-optimization-problem-solving">Levels of Optimization Problem Solving</a></div> <div><a href="#bayesian-optimization">Bayesian Optimization</a></div> <div><a href="#implementation">Implementation</a></div> </nav> </d-contents> <h1 id="opening-discussion">Opening Discussion</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bayesian_opt/open_discussion-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bayesian_opt/open_discussion-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bayesian_opt/open_discussion-1400.webp"></source> <img src="/assets/img/bayesian_opt/open_discussion.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="levels-of-optimization-problem-solving">Levels of Optimization Problem Solving</h1> <p>In general, we can divided the way to solve optmization problem into \(3\) levels:</p> <ul> <li> <strong>Level 0</strong>: Non-derivative, including some familiar approaches such as: Grid Search, Random Search, Bayesian Optimization, etc. <ul> <li>Pros: Fast computation, do not require the function to be smooth, i.e. no differentiable required, the function need continous</li> <li>Cons: It is heuristic, not a theory-based method</li> </ul> </li> <li> <strong>Level 1</strong>: First-derivative. \(∇f(x) = f(x_i)_i\). The most popular one is Gradient Descent (Adam, Nesterov, etc.) <ul> <li>Pros: Efficient to converge in local minimum/maximum</li> <li>Cons: Computational since it requires backpropagation</li> </ul> </li> <li> <strong>Level 2</strong>: Second-derivatie. \(J(f) = f(x_i, x_j)_{ij}\) (i.e. Newton) <ul> <li>Pros: Very effective, faster convergence than level-1 method</li> <li>Cons: Significantly compuational</li> </ul> </li> </ul> <p>Bayesian Optimization is the one in level 0</p> <h1 id="bayesian-optimization">Bayesian Optimization</h1> <h2 id="when-to-use-bayesian-optimization">When to use Bayesian Optimization</h2> <p>Supose that we are optimzing a function \(f(x, \Theta)\) \(\max_{\Theta}f(x,\Theta)\)</p> <p>We can consider Bayesian Optimzation if $f$ satisfy following conditions:</p> <ul> <li>$f$ has to be continuous</li> <li>$f$ is expensive to evaluate</li> <li>$f$ is a blackbox. In other words, we don’t know characteristic of $f$ (convex, non-convex, derivative, etc.)</li> </ul> <h2 id="fundementals-of-bayesian-optmization">Fundementals of Bayesian Optmization</h2> <p>Instead of directly optimizing $f$, we approximate it by other easier evaluation function \(P\), and define a strategy $u$ to optimize \(P\)</p> <ul> <li>\(P\) - <strong>Surrogate model</strong>: <ul> <li>the alternative of \(f\), easier to optmize</li> <li>Gaussian Process, etc.</li> </ul> </li> <li>\(u\) - <strong>Acquisition function</strong>: <ul> <li>strategy to optmize surrogate model</li> <li>Expected Improvement (EI), Upper Condidence Bound (UCB), etc.</li> </ul> </li> </ul> <p>In this article, we focus on the most popular surrogate model - the Gaussian Process</p> <h2 id="gaussian-process">Gaussian Process</h2> <p>As mentioned, \(f\) is black box and expensive to evaluate. Hence, the surrogate model should be an alternative that is easier to evaluate. A good intuition is to approximate \(f\) as a distribution (normally Gaussian Distribution).<br> In other words, we consider \(f\) as a Gaussian Distribution with mean \(\mu(x)\) and standard deviation \(\sigma^2(x)\)</p> \[f(x) \sim \mathcal{N}(\mu(x), \sigma^2(x))\] <p>Hence, the Gaussian Process \(P\) has the Probability Density Function (PDF) as</p> \[P(f(x)=y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\frac{|y - \mu(x)|^2}{2\sigma^2}}\] <p>The figure below demonstrates it as a distribution approximation </p> <p>Therefore, Gaussian Process makes use of Bayes Theory. Since we literally has no information about how \(f\) looks like (<em>the solid red line</em>), we just has some initial observations (<em>blue circle</em>). <strong>The surrogate model gives us a probabilistic estimation of $f$ as a distribution (<em>the area limited by dash red line</em>)</strong></p> <p>From this, we need to select the next point (i.e. \(x_+\)) to sample from \(f\). The strategy to select the next point from the observation of surrogate model is taken by <strong>acquisition function</strong>.</p> <p>After the next point is sampled, i.e. \(\{x_+, y=f(x_+)\}\). We continue to fit it as prior knowledge to surrogate model and continue to select next point, i.e. \(x_{++}\). Gradually, we will have a more knowledge on how \(f\) looks like as the belief keeps increasing.</p> <h2 id="acquisition-function">Acquisition function</h2> <p>The strategy that proposes the next sampling point is decided by acquisition function \(u\). Particularly, we optimize \(u\) over surrogate model \(P\). Note that as mentioned, those functions are easier to optimize comparing to \(f\)<br> There are lots of <strong>acquisition function</strong>, the most popular ones are <strong>Expected Improvement</strong> \(EI\), and <strong>Upper Confidence Bound</strong> \(UCB\). In this article, we define \(EI\) as the acquisition function.</p> <p>Expected Improvement function is defined as \(EI(x) = \mathbb{E} \max (x - f(x^*))\)</p> <p>The intuition is that the chosen next sampling point should have maximal expected values comparing the best of those from observations. The best point from existing observations is denoted as \(x^* = \arg\max_{x_i \in D} f(x_i)\)</p> <p>Since we are optimizing acquisition function over a Gaussian Process, the more particular formula can be clarified as \(EI(x) = \begin{cases} \displaystyle (\mu(x) - f(x^*) - \xi)\varphi(Z) + \sigma(x)\phi(Z) &amp; \text{if} &amp; \sigma(x) &gt; 0 \\ \displaystyle 0 &amp; \text{if} &amp; \sigma(x) = 0 \end{cases}\)</p> <p>where</p> \[Z = \begin{cases} \displaystyle \frac{\mu(x) - f(x^*) - \xi}{\sigma(x)} &amp; \text{if} &amp; \sigma(x) &gt; 0 \\ \displaystyle 0 &amp; \text{if} &amp; \sigma(x) = 0 \end{cases}\] <p>The adjusted parameter \(\xi\) is use to balance between <em>exploitation</em> and <em>exploration</em> of two summation terms in the above equation, respectively.</p> <p>High \(\xi\) means we lower the probability of first term (the one calculating with mean \(\mu\), i.e. the certainty), hence, increase the probability of second term (the one calculating with variance \(\sigma\), i.e. the uncertainty) –&gt; we want to explore on the area of uncertainty more, and vice versa.</p> <h2 id="the-complete-process">The complete process</h2> <p>In summary, the overall process of Bayesian Optimization can be clarified as follows: We want to find the optimal values of objective function \(f\) with <strong>surrogate model</strong> \(P\) and <strong>acquisition function</strong> \(u\). Initially, we have limited observations \(D={x_N, y_N}\). The iteration below is how Bayesian Optimization works:<br> — <br> 1. Fit \({x_N, y_N}\) to approximate surrogate model \(P\)<br> 2. Optimize acquisition function \(u\) over \(P\) to sample the next point \(x_{N+} = u(x)\)<br> 3. Evaluate \(\{x_{N+}\}\) on \(f\) to get \(y_{N+}\)<br> 4. Add \(\{x_{N+}, y_{N+}\}\) to D and repeat the process</p> <p>The detail about implementation from scratch can be found at <a href="http://krasserm.github.io/2018/03/19/gaussian-processes/" rel="external nofollow noopener" target="_blank">[4]</a></p> <p>Below is the demonstration step-by-step:</p> <p>Suppose we want to find the values that maximize the objective function below (which is unknown) with some initial observations. </p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bayesian_opt/process_step1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bayesian_opt/process_step1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bayesian_opt/process_step1-1400.webp"></source> <img src="/assets/img/bayesian_opt/process_step1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>Approximate a surrogate model \(P\) (i.e. Gaussian Process). The green area is CI - Confidence Interval drawn from \(\sigma^2\) of surrogate model, depicts the uncertainty over objective function </li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bayesian_opt/process_step2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bayesian_opt/process_step2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bayesian_opt/process_step2-1400.webp"></source> <img src="/assets/img/bayesian_opt/process_step2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>Apply acquisition function \(u\) over \(P\). As a result, it proposed the next sampling point as blue circle </li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bayesian_opt/process_step3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bayesian_opt/process_step3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bayesian_opt/process_step3-1400.webp"></source> <img src="/assets/img/bayesian_opt/process_step3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>Evaluate the proposed sampling point using objective function. We can observe that the uncertainty significantly narrow down. We repeat this process through a number of iterations </li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bayesian_opt/process_step4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bayesian_opt/process_step4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bayesian_opt/process_step4-1400.webp"></source> <img src="/assets/img/bayesian_opt/process_step4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="discussion">Discussion:</h2> <p>We may have a general observation about Bayesian Optimization. However, there are still some that we have resolve yet:</p> <ul> <li>Alternatively, we optimize acquisition function over surrogate model instead of objective function since it cheaper, yet it is not clear the approach to optimize acquisition function</li> <li>Beside Expected Improvement, other popular acquisition functions are UCB, Probability of Improvement</li> </ul> <h1 id="implementation">Implementation</h1> <p>To simplify the implementation, we suppose that:</p> <ul> <li>We have already known the objective function for the verification.</li> <li>The surrogate model makes use of Gaussian Process Regression provided by <code class="language-plaintext highlighter-rouge">scikit-learn</code> </li> <li>The process of optimizing the acquisition function will be handled by <code class="language-plaintext highlighter-rouge">scipy</code> employing <em>Broyden–Fletcher–Goldfarb–Shanno</em> algorithm[6]</li> </ul> <p>We will implement</p> <ul> <li>The Bayesian Optimization process</li> <li>A method maximizing acquisition function</li> <li>An instance of acquisition function, i.e. Expected Improvement</li> </ul> <h2 id="set-up">Set up</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TO-DO: problem with scipy &gt; 1.4, find out solution
</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">unistall</span> <span class="n">scipy</span> <span class="o">--</span><span class="n">y</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">scipy</span><span class="o">==</span><span class="mf">1.4</span><span class="p">.</span><span class="mi">1</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># assume the objective function f
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Initial samples
</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span>
<span class="n">X_inits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">]])</span>
<span class="n">y_inits</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">X_inits</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">X_inits</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_inits</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Bounds of x
</span><span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(2, 1) (2, 1)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># enrich samples to draw objective function
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="mf">0.01</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">noise</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">The objective function and initial observations: </span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">y--</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Objective function</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="p">),</span> <span class="sh">'</span><span class="s">gx</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Noisy samples</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_inits</span><span class="p">,</span> <span class="n">y_inits</span><span class="p">,</span> <span class="sh">'</span><span class="s">kx</span><span class="sh">'</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Initial samples</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The objective function and initial observations: 
</code></pre></div></div> <h2 id="bayesian-optimization-1">Bayesian Optimization</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># !wget !wget https://raw.githubusercontent.com/krasserm/bayesian-machine-learning/dev/bayesian-optimization/bayesian_optimization_util.py
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="n">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ConstantKernel</span><span class="p">,</span> <span class="n">Matern</span>
<span class="kn">from</span> <span class="n">bayesian_optimization_util</span> <span class="kn">import</span> <span class="n">plot_approximation</span><span class="p">,</span> <span class="n">plot_acquisition</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_bayesian_opt</span><span class="p">(</span><span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">gpr</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Single iteration of a Bayeisan Optimization process
    Args:
        X_samples: 
        y_samples: 
        gpr: 
        
    Returns:
        Next sampling pair {X, y}
    </span><span class="sh">"""</span>
    
    <span class="c1"># approximate GPR
</span>    <span class="n">gpr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">)</span>

    <span class="c1"># optimize acquisition over GPR to chose next sampling point X_next
</span>    <span class="n">X_next</span> <span class="o">=</span> <span class="nf">optimize_acquisition</span><span class="p">(</span><span class="n">EI</span><span class="p">,</span> <span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">n_restarts</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

    <span class="c1"># evaluate to get y_next
</span>    <span class="n">y_next</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">X_next</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">y_next</span>
</code></pre></div></div> <h2 id="optimize-acquisition">Optimize acquisition</h2> <p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">scipy.optimize.minimize</code></a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="k">def</span> <span class="nf">optimize_acquisition</span><span class="p">(</span><span class="n">acquisition_func</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">n_restarts</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Optimize acquisition function
    Args:
        acquisition_func: 
        X_sample: 
        y_sample: 
        gpr: 
        bounds:
        n_restarts:
        
    Returns:
        proposed location
    </span><span class="sh">"""</span>
    
    <span class="n">dim</span> <span class="o">=</span> <span class="n">X_sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">min_value</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">min_x</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">minimize_objective</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="nf">acquisition_func</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">dim</span><span class="p">),</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">)</span>
    
    <span class="c1"># minimize by a popular algorithm
</span>    <span class="k">for</span> <span class="n">x0</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_restarts</span><span class="p">,</span> <span class="n">dim</span><span class="p">)):</span>
        <span class="n">min_ei</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">minimize_objective</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">L-BFGS-B</span><span class="sh">'</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">min_ei</span><span class="p">.</span><span class="n">fun</span> <span class="o">&lt;</span> <span class="n">min_value</span><span class="p">:</span>
            <span class="n">min_value</span> <span class="o">=</span> <span class="n">min_ei</span><span class="p">.</span><span class="n">fun</span>
            <span class="n">min_x</span> <span class="o">=</span> <span class="n">min_ei</span><span class="p">.</span><span class="n">x</span>
    
    <span class="k">return</span> <span class="n">min_x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="acquisition_func-expected-improvement"> <code class="language-plaintext highlighter-rouge">acquisition_func</code>: Expected Improvement</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">EI</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Expected Improvement acquisition function
    Args:
        X
        X_sample: 
        y_sample: 
        gpr:
        xi: 
    
    Returns
        EI at X
    </span><span class="sh">"""</span>
    
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">gpr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">mu_sample</span> <span class="o">=</span> <span class="n">gpr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>
    
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># find f(x^*)
</span>    <span class="n">mu_max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">mu_sample</span><span class="p">)</span>
    
    <span class="c1"># EI formula
</span>    <span class="k">with</span> <span class="n">np</span><span class="p">.</span><span class="nf">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="sh">'</span><span class="s">warn</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">mu_dst</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">mu_max</span> <span class="o">-</span> <span class="n">xi</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">mu_dst</span> <span class="o">/</span> <span class="n">sigma</span>
        <span class="n">ei</span> <span class="o">=</span> <span class="n">mu_dst</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        <span class="n">ei</span><span class="p">[</span><span class="n">sigma</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
    
    <span class="k">return</span> <span class="n">ei</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m52</span> <span class="o">=</span> <span class="nc">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="nc">Matern</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">gpr</span> <span class="o">=</span> <span class="nc">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">X_samples</span> <span class="o">=</span> <span class="n">X_inits</span>
<span class="n">y_samples</span> <span class="o">=</span> <span class="n">y_inits</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>    
    <span class="n">X_next</span><span class="p">,</span> <span class="n">y_next</span> <span class="o">=</span> <span class="nf">process_bayesian_opt</span><span class="p">(</span><span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">gpr</span><span class="p">)</span>
    
    <span class="c1"># Plot samples, surrogate function, noise-free objective and next sampling location
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">n_iters</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nf">plot_approximation</span><span class="p">(</span><span class="n">gpr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Iteration </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">n_iters</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="nf">plot_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nc">EI</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_samples</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">,</span> <span class="n">gpr</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># Add sample to previous samples
</span>    <span class="n">X_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">X_samples</span><span class="p">,</span> <span class="n">X_next</span><span class="p">))</span>
    <span class="n">y_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">y_samples</span><span class="p">,</span> <span class="n">y_next</span><span class="p">))</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">bayesian_optimization_util</span> <span class="kn">import</span> <span class="n">plot_convergence</span>

<span class="nf">plot_convergence</span><span class="p">(</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">)</span>
</code></pre></div></div> <h1 id="references">References</h1> <p>[1] A Tutorial on Bayesian Optimization: https://arxiv.org/pdf/1807.02811.pdf<br> [2] <a href="https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0" rel="external nofollow noopener" target="_blank">The intuitions behind Bayesian Optimization with Gaussian Processes</a><br> [3] <a href="https://orbi.uliege.be/bitstream/2268/226433/1/PyData%202017_%20Bayesian%20optimization%20with%20Scikit-Optimize.pdf" rel="external nofollow noopener" target="_blank">Bayesian optimization with Scikit-Optimize</a><br> [4] <a href="http://krasserm.github.io/2018/03/21/bayesian-optimization/" rel="external nofollow noopener" target="_blank">Bayesian Optimization</a><br> [5] <a href="http://krasserm.github.io/2018/03/19/gaussian-processes/" rel="external nofollow noopener" target="_blank">Gaussian processes</a><br> [6] <a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" rel="external nofollow noopener" target="_blank">Broyden–Fletcher–Goldfarb–Shanno algorithm</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/bayesian-optimization.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"tranminhquan/tranminhquan.github.io","data-repo-id":"R_kgDOJ8OIkw","data-category":"Q&A","data-category-id":"DIC_kwDOJ8OIk84CX792","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Quan M. Tran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>