{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-supervised learning method for Heterogeneous Graph\n",
    "\n",
    "\n",
    "## Modeling\n",
    "Follow the concept of autoencoder\n",
    "\n",
    "* Encoder $f_{\\theta}$ using GNNs backbone, encode each $v_i \\in \\mathcal{V}$ into $h_i \\in H$\n",
    "* Pretext decoder $p_{\\phi}$ takes $H$ as input for prextext task  \n",
    "    *Pretext task is a definition of a problem that the model is optimized based on it in the concept of self-supervised learning*\n",
    "\n",
    "After the SSL training, the trained model is used for downstreaming task\n",
    "* $q_{\\psi}$ is downstream decoder\n",
    "\n",
    "\n",
    "The objective functions include\n",
    "* Objective function for Graph SSL\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\argmin_{\\theta, \\phi} \\mathcal{L_{ssl}}(f_{\\theta}, p_{\\phi}, \\mathcal{D})\n",
    "$$\n",
    "where $(\\mathcal{V}, \\mathcal{E}) \\sim \\mathcal{D}$\n",
    "\n",
    "\n",
    "* Objective function for Graph SL (downstreaming task)\n",
    "$$\n",
    "\\theta^{**}, \\psi^* = \\argmin_{\\theta^*, \\psi} \\mathcal{L_{sup}}(f_{\\theta^*}, q_{\\psi}, \\mathcal{G}, y)\n",
    "$$\n",
    "where $y$ is the label\n",
    "\n",
    "\n",
    "## Self-supervised learning methods\n",
    "### Categories\n",
    "Therer are 4 categories:\n",
    "* Generation-based: this kind of method reconstruct the graph data by either *features* or *graph structure*.  \n",
    "Hence $\\mathcal{L_{ssl}}$ is usually defined to measure the difference between the original and reconstructed graph data. The representative is GAE. \n",
    "* Auxiliary Property-based: this kind of method define the pretext task following regression or classification. The labels are pseudo from graph itself. For instance,  \n",
    "    * regression problem optimizes based the node degress, or distance to the cluter. Hence $\\mathcal{L_{ssl}}$ can be MSE\n",
    "    * classification problem optimized based on graph partitions, cluster indices. Hence $\\mathcal{L_{ssl}}$ can be CE$\n",
    "\n",
    "    The representative for this is M3S\n",
    "\n",
    "* Contrastive-based method: the original $\\mathcal{G}$ is augmented into $\\mathcal{G}^1$ and $\\mathcal{G}^2$. $\\mathcal{L_{ssl}}$ is defined as contrastive loss, which is similar to the idea of triplet loss. This is call *Mutual Information (MI)* maximization. Which is, the objective function maximizes MI the positive pairs of two augmented graphs and vice versa for the negative pairs\n",
    "\n",
    "    Representive for this are DGI, GraphCL, GCC\n",
    "\n",
    "\n",
    "* Hybrid: combine the above methods using weighted or unweighted averaging. The representive is GMI, optimize the reconstruction of edge level (generation-based) and the contrast of node level (contrastive-based)\n",
    "\n",
    "\n",
    "### Objective function formulation\n",
    "#### Generation-based method\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\argmin_{\\theta, \\phi} \\mathcal{L_{ssl}}(p_{\\phi}(f_{\\theta}(\\hat{\\mathcal{G}})), \\mathcal{G})\n",
    "$$\n",
    "TODO: GAE brief\n",
    "\n",
    "There are 2 sub-categories: reconstruct the *features*, or, reconstruct the *structure*\n",
    "\n",
    "##### Reconstruct the features\n",
    "The target objective function is to minimize the generated vs original features, which is\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\argmin_{\\theta, \\phi} \\mathcal{L_{ssl}}(p_{\\phi}(f_{\\theta}(\\hat{\\mathcal{G}})), \\hat{X})\n",
    "$$\n",
    "\n",
    "$\\hat{X}$ is the definition of features such as node feature matrix, edge feature matrix, low-dimension feature, etc.\n",
    "\n",
    "Inspired from inpainting problem in Computer Vision, *masked feature regression* is applied in graph. The adjacency matrix $A$ is masked with a binary matrix. Then, the model learn to reconstruct the masked graph to the original one. **Hence, the representation often captures the node-level knowledge**\n",
    "\n",
    "Hence, from the above Equation, $\\hat{G} = (A, \\hat{X})$, and $\\hat{X} = X$, which is, the encoder learns from a masked original graph data, and predict the masked nodes / edges attributes using the neighbor information. As a result, this method requires to train the graph as a whole, or a large sub-graph to ensure the effective learning. The representive is *Graph Completion*. Additionally, methods such as AttrMasking learns from both nodes and edges attributes, then, $\\hat{X} = [X, X_{edge}]$\n",
    "\n",
    "Other method finds it hard to learn from orignal attributes, these are transformed into a lower dimension instead, which can be PCA (*AttributeMask*). Hence, $\\hat{X} = PCA(X)$.\n",
    "\n",
    "As generation-based method usually relies on Autoencoder architecture, some variants of its can be applicable, such as denoising autoencoder. The representative is *MGAE*. Hence, $\\hat{G} = (A, \\bar{X})$, where $\\bar{X} = \\hat{X} + S_{noise}$ where $S_{noise} \\sim \\mathcal{N}$ is random noise added.\n",
    "\n",
    "TODO: Graph Completion, AttributeMask, MGAE\n",
    "\n",
    "\n",
    "##### Reconstruct the structure\n",
    "The objective function is as follow\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\argmin_{\\theta, \\phi} \\mathcal{L_{ssl}}(p_{\\phi}(f_{\\theta}(\\hat{\\mathcal{G}})), A)\n",
    "$$\n",
    "\n",
    "The idea is to reconstruct the topology of knowledge graph which is represents by adjeacency matrix $A$. The encoder learns from $\\hat{\\mathcal{G}}$ to $H$, then the decoder uses $H$ to reconstruct the node features. The reconstruction of structure is measure by predicting whether there is an edge between nodes or not. **Hence, the representation often captures the node-pair level information**\n",
    "\n",
    "Particularly, the problem relaxes as a binary classification and can use the BCE to optimize it. The representative is *GAE*, other variants are *VGAE*, *SIG-VAE*, *ARGA / ARVGA* (integrated with GAN concept). This kind of method can suffer the imbalance problem, but can be addressed by re-weighting, or oversampling the minority classes.\n",
    "\n",
    "Instead of learning and predicting the whole nodes in the graph, other methods randomly drop some edges, then the model learns to generalize those edges. BCE is also applied to learn and minimize between connected nodes. The representatives are *Denoising Link Reconstruction*, *EdgeMask*\n",
    "\n",
    "\n",
    "### Auxiliary Property-based method\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\argmin_{\\theta, \\phi} \\mathcal{L_{ssl}} (p_{\\phi}(f_{\\theta}(\\mathcal{G})), c)\n",
    "$$\n",
    "\n",
    "where $c$ is the properties that are designed based on whether the problem is regression or classification  \n",
    "TODO: M3S brief\n",
    "\n",
    "\n",
    "#### Classification task\n",
    "##### Clustering: \n",
    "define the pseudo label for each node by a mapping function $\\Omega : \\mathcal{V} \\to \\mathcal{C}$.\n",
    "\n",
    "$\\Omega$ is one of clustering methods\n",
    "\n",
    "> My opinion: this method significantly relies on the clustering algorithm unless $\\Omega$ is trained along with graph model\n",
    "\n",
    "The particular optimization can be clarified as\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\frac{1}{|\\mathcal{V}|}\\sum_{\\mathcal{v}_i \\in \\mathcal{V}}^{} \\mathcal{L}_{ce} (p_{\\phi}(f_{\\theta}(\\mathcal{\\mathcal{G}_{\\mathcal{v}_i}})), \\Omega(\\mathcal{v}_i))\n",
    "$$\n",
    "\n",
    "##### Pair-wise\n",
    "focus to define the pseudo labels for pairs of nodes rather then nodes themselves. Then $\\Omega: \\mathcal{V} \\times \\mathcal{V} \\to \\mathcal{C}$\n",
    "\n",
    "$\\Omega$ can be defined based on the distance between nodes, or number of hops, etc.\n",
    "\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\frac{1}{|\\mathcal{P}|}\\sum_{\\mathcal{v}_i, \\mathcal{v}_j \\in \\mathcal{P}}^{} \\mathcal{L}_{ce} (p_{\\phi}(f_{\\theta}(\\mathcal{G}_{(\\mathcal{v}_i, \\mathcal{v}_j)})), \\Omega(\\mathcal{v}_i, \\mathcal{v}_j))\n",
    "$$\n",
    "\n",
    "where $\\mathcal{P} \\subseteq \\mathcal{V} \\times \\mathcal{V}$\n",
    "\n",
    "Note that this method also belongs to the classification, then the peuso labels are discreate\n",
    "> My opinion: This method can learn from the structure of the graph, which is a very good learning strategy, however, the computation cost would increase if size of the model is large due to the pair-wise generation.\n",
    "\n",
    "\n",
    "#### Regression task\n",
    "The difference compared with the classification task is the pseudo labels are continuous and optimized based on MSE-based loss functions. The pseudo labels can be generated from node degrees, the local importance, etc. Similar to the classification task, the learning context can be either node-centric or pairwise-centric. In the later, the pseudo labels can be the similarity of their (raw) attributes between nodes.\n",
    "\n",
    "For the node-centric regression, the objective function is\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\frac{1}{|\\mathcal{V}|}\\sum_{\\mathcal{v}_i \\in \\mathcal{V}}^{} \\mathcal{L}_{mse} (p_{\\phi}(f_{\\theta}(\\mathcal{\\mathcal{G}_{\\mathcal{v}_i}})), \\Omega(\\mathcal{v}_i))\n",
    "$$\n",
    "\n",
    "For the pairwise-centric regression, the objective function is\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\frac{1}{|\\mathcal{P}|}\\sum_{\\mathcal{v}_i, \\mathcal{v}_j \\in \\mathcal{P}}^{} \\mathcal{L}_{mse} (p_{\\phi}(f_{\\theta}(\\mathcal{G}_{(\\mathcal{v}_i, \\mathcal{v}_j)})), \\Omega(\\mathcal{v}_i, \\mathcal{v}_j))\n",
    "$$\n",
    "\n",
    "### Contrastive-based method\n",
    "$$\n",
    "\\theta^*, \\phi^* = \\argmin_{\\theta, \\phi} \\mathcal{L_{ssl}} (p_{\\phi}(f_{\\theta}(\\hat{\\mathcal{G}^1}), f_{\\theta}(\\hat{\\mathcal{G}^2})))\n",
    "$$\n",
    "\n",
    "\n",
    "# References\n",
    "https://arxiv.org/pdf/2103.00111.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
